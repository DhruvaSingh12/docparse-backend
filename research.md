Design and Development of an LLM-Powered Document Parsing Pipeline for Automated Medical Data Extraction and Validation

 
Dhruva Singh 
Dept. of Computer Science and Engineering
SRMIST Delhi-NCR Campus
Ghaziabad, India
singhdhruva45@gmail.com
https://orcid.org/0009-0000-3213-9187 
 
 
 
Abstract—Background: Processing medical documents by hand, and in the context of healthcare reimbursements, is a major bottleneck that carries exorbitant costs, long waits, and high rates of human error. This work details the design, development, and evaluation of a document parsing pipeline utilizing an LLM for automatically extracting and validating data from Indian medical bills. I implement a nuanced multi-stage approach to solve the problem of unstructured, variable-layout documents. For this task, I labelled a custom open dataset of 395 images with 8 entity classes.    Method:  The pipeline is constructed on a modified ensemble detection model consisting of a Weighted Box Fusion (WBF) of 2 deep learning architectures, YOLOv11n and Faster RCNN, to accurately and precisely detect regions. The Optical Character Recognition (OCR) step extracts text after detection of the targeted regions’ coordinates, which uses PaddleOCR as the primary engine and Tesseract as a fallback. Complicated tabular data is parsed using the Microsoft Table-Transformer model. The next step is the post-processing loop, which employs Google's MedGemma 4B, a 4-bit quantized multimodal model for domain-specific validation, semantic correction, and intelligent text parsing, after the text is extracted. The entire pipeline runs over an async FastAPI, and the extracted text records are stored in a serverless PostgreSQL database. 
Results: The ensemble model achieved a Macro F1-score of 0.906 at a 0.5 IoU threshold. The pipeline is capable of processing a single document in ~15-20 seconds.
Conclusion: Utilizing the pipeline, a medical bill can be parsed, extracting the required fields and storing them after validation in a serverless database.

Keywords—Medical bills, OCR, Ensemble model, Object detection, YOLOv11, Faster R-CNN, MedGemma, Extraction, Validation, Document parsing.

I.	INTRODUCTION
The healthcare industry faces an enormous challenge of processing medical bills and insurance claims, which is traditionally undergone via manual and labour-intensive techniques. This results in possible errors and high costs, sometimes leading to denials and rejections of claims and reimbursements after an overall lengthy procedure. Automated document parsing pipelines can minimize this manual burden and perform more efficiently by extracting structured relevant records from semi or unstructured formats. However, healthcare documents are highly variable in their formats and noisy images/scans can limit normal OCR based pipelines. I tried to address this by designing and implementing an end-to-end OCR-LLM based pipeline that can understand context, detect positioning of targeted fields, extract those relevant fields and finally store them in a structured, uniform format in a serverless database. Some key features of the proposed pipeline include (1) an ensemble model to detect the regions with the fields of interest; (2) a dual-engine OCR and table-parsing for text extraction; (3) an LLM based layer for sematic validation and correction of outputs; and finally (4) the ability to run locally on lightweight hardware. This paper highlights each step, ranging from data collection/preprocessing, models’ training, OCR and LLM integration to storing results in a serverless database and how the stated implementation can significantly improve the efficiency over the current legacy methods.

II.	RELATED WORKS
A.	Traditional and OCR Based Document Understanding
Previous efforts towards document parsing have relied on handcrafted features and rule-based methods for text detection and extraction, usually performing poorly on noisy or unstructured layouts. Traditional OCR systems such as Tesseract and some CNN-RNN hybrid models improved performances on printed texts but lacked enough flexibility to handle variably positioned fields or multilingual contents, especially in unstructured documents such as medical bills. Various neural network-based parsers, like CRAFT[1] and PixelLink, improve spatial text localization in scanned documents. Tools incorporating both visual and positional cues, such as LayoutParser and LayoutLM, have improved the semantic understanding of the structure in documents. However, they still heavily rely on the quality of the input given by OCR, limiting robustness on low-quality scans or handwritten documents.

B.	Hybrid Visual-Semantic Approaches
Several works explored deep object detection models to identify key regions in documents as a means to overcome the limitations of template-based methods. While Faster R-CNN-based pipelines provided higher accuracy at the expense of latency, their YOLO-based counterparts provided high inference speed. Especially for variable-layout forms, ensemble methods like Weighted Boxes Fusion showed promise in combining the strengths of multiple detectors. Dual-stage OCR processing with confidence-based fallback mechanisms and domain-specific layout analysis using table parsers like Table-Transformer from Microsoft were introduced by more sophisticated pipelines. These systems performed well on structured documents but, due to script variability, multilingual content, and noisy backgrounds, often did not generalize well across a range of formats such as Indian medical bills.

C.	LLM-Driven Post-OCR Validation
VLMs and LLMs have also recently been used for OCR correction and semantic validation. Examples include Donut, Pix2Struct, and Dessurt, which illustrate that with the addition of layout reasoning and contextual understanding, the outputs can be more structured, often bypassing traditional OCR altogether. However, these models are very computationally heavy and require end-to-end fine-tuning, which may not always be feasible for domain-specific, low-resource datasets. Previous work in the Indian medical context has relied on using Indic language models, and script-aware tokenizers to support Hindi and regional OCR tasks. However, most of these models either are incapable of handling entire documents holistically or are domain-specific and lack correction logic that can be easily generalized across domains. Complementary to these trends is my approach of introducing a lightweight, rule-guided pre-validation layer followed by a quantized, multimodal LLM fine-tuned for the medical domain, MedGemma. Unlike general-purpose models, MedGemma allows for semantic correction, contextual reformatting, and validation of OCR-extracted data, enhancing reliability without extensive retraining.

III.	DATA COLLECTION AND DETECTION MODELS

A.	Data Collection, Labeling, Augmentation and Splitting
I found a publicly available dataset of 395 Indian medical bills on the Roboflow Universe with some basic annotations, but they did not align with my requirements. Therefore, I forked the dataset into my workspace and redefined the annotation schema to include the following eight classes: (1) date_of_receipt, (2) gstin, (3) invoice_no, (4) mobile_no, (5) product_table, (6) store_address, (7) store_name and (8) total_amount. I then manually re-annotated all the images using Roboflow’s bounding-box annotation tool over the newly defined labels. 
So as to increase generalization I applied two data-augmentation techniques: (1) image rotation (clockwise and anti-clockwise by 90°) to stimulate orientation variance and (2) grayscale filtering on randomly selected 15% of the images to mimic a poor scan or a faded print. After these augmentations I had an expanded dataset of 862 images which was split into training: 742 images (~86.1%), validation: 60 images (~7%) and testing: 60 images (~7%) sets. Finally, this dataset was exported in YOLOv11 and COCO formats to train two distinct object detection architectures.

B.	YOLOv11n Model Training
Initially, I trained a YOLOv11n (nano variant) object detection model. Training ran for 100 epochs with a batch size of 16 on CUDA-enabled GPU with an image input resolution of 640×640 pixels, and the patience of early stopping was set to 50 epochs for preventing overfitting. The architecture of YOLOv11n consists of an efficient nano-scale backbone optimized for mobile and edge deployment with scaling factors of depth and width. CSPDarknet-inspired feature extraction in the form of multi-scale detection with PANet-style feature pyramid networks uses anchor-free decoding with distribution focal loss (DFL) for accurate bounding box regression.

The optimization strategy utilized stochastic gradient descent with a cosine annealing learning rate schedule, initialized at 0.0008276, linearly decaying to 0.0000166 by epoch 100, applied uniformly across all three parameter groups: pg0, pg1, pg2. The training pipeline utilized comprehensive data augmentation strategies: (1) mosaic augmentation of four images, (2) mixup through blending image pairs, (3) HSV color space transformations in hue, saturation and value jittering, (4) random horizontal flips, (5) scale jittering, (6) translation augmentation, and (7) affine transformation to improve model generalization and robustness to lighting variation, orientation change, and document quality degradation. The multi-component loss function combined box regression loss based on CIoU for precise localization, classification loss as binary cross-entropy with focal loss weighting to balance class imbalance, and DFL for refining the corners of bounding boxes in a finer granularity, with final training losses converging to 1.028 for box, 0.703 for class, and 0.924 for DFL.

Training convergence saw steady improvement across all metrics, with precision increasing from 0.19% in epoch 1 to 88.76% in epoch 100, recall improving from 11.64% to 87.93%, and mAP50 rising from 4.76% to 90.73%, whereas mAP50-95 was 60.70%, showing excellent performance at the multi-threshold. Convergence in validation losses went down from starting values of 2.353 for box, 3.947 for class, and 1.420 for DFL to finishing converged values of 1.092, 0.674, and 0.933, respectively. On this test set alone, the model attained 90.23% precision, 88.25% recall, and 92.17% mAP50 with exceptionally high performance on product tables and total amounts at 99.5% mAP50/100% recall and 99.48% mAP50/100% recall, respectively, while mobile number detection presented difficulties, 64.24% mAP50 and 49.12% recall, because of their varied formatting and the complexity of OCR-style recognition. The weights had been saved on epoch 88, where the validation mAP50 value peaked at 90.87%, proving how very well YOLOv11n was suited for real-time document understanding tasks with very low computational overhead and, correspondingly, ready-for-deployment features.

C.	Faster R-CNN (ResNet-50 FPN) Model Training
Next, I trained a Faster R-CNN object detection model, using the ResNet-50 Feature Pyramid Network backbone, in PyTorch TorchVision. The model underwent training for 25 epochs with a batch size of 4 on CUDA-enabled GPU using COCO-pretrained ResNet-50 FPN weights by way of transfer learning, whereby pre-learned hierarchical feature representations from natural images greatly accelerated model convergence and improved generalization to document-domain object detection.

The Faster R-CNN architecture adopts a two-stage detection pipeline wherein a class-agnostic region proposal is generated through anchor-based sliding window mechanisms by the RPN, followed by a subsequent classification and refinement of the bounding box through the ROI head. A ResNet-50 FPN backbone extracts multi-scale feature pyramids using a bottom-up pathway-a ResNet convolution with residual connections-followed by a top-down pathway through lateral connections with upsampling, allowing it to detect objects across varied scale ranges, which is critical in the case of medical bills, where field sizes range from small mobile numbers to large product tables. The model architecture embeds four kinds of specialized losses: (1) RPN classification loss-a binary cross-entropy-to provide objectness scores; (2) RPN bounding box regression loss-a smooth L1-for the RPN to arrive at proper proposal localizations; (3) ROI classifier loss-a cross-entropy-to calculate final class predictions; and (4) ROI box regression loss-smooth L1-for obtaining very accurate coordinate refinements.

The optimization strategy used SGD with momentum of 0.9 and L2 weight decay regularization of 0.0005 to prevent overfitting, starting from an initial learning rate of 0.005. Furthermore, the step-based learning rate schedule was applied with the step size of 10 epochs and decay factor (gamma) of 0.1, reducing the learning rate to 0.0005 at epoch 10 and further to 0.00005 at epoch 20 to enable fine-grained convergence. Training convergence showed a rapid initial improvement followed by steady refinement: the total loss decreased from 0.931 (epoch 1) to 0.149 (epoch 25), and the component losses stabilized at 0.0285 (classifier), 0.0669 (box regression), 0.0074 (objectness), and 0.0462 (RPN box regression). The loss decomposition shows that RPN components converged faster than the ROI head losses, which means effective proposal generation while classification and localization required extended fine-tuning. Standard COCO-style normalization with ImageNet statistics (mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225]) was applied via torchvision transforms to fit the pretrained ResNet-50 backbone weights.

The final evaluation metrics showed very strong performance with mAP50 of 88.27%, mAP75 of 66.92%, and mAP50-95 of 58.67%, which indicated the robustness in the accuracy of object localization. In more detail, it achieved a macro-averaged precision of 78.61%, recall of 79.89%, and F1 score of 79.17%, with an accuracy in detection of 80.71%. The best weights of the model were saved at epoch number 24, showing its suitability for high-accuracy document understanding tasks where precise multi-scale object localizations are needed.

D.	Weighted Box Fusion Ensemble Model
After training the two models I created an ensemble model that combines the Faster R-CNN with YOLOv11n, using Weighted Box Fusion to take advantage of the complementary strengths of both architectures in improving the accuracy of medical bill field detection. The ensemble strategy followed addressed the fundamental trade-off between the two base models: Faster R-CNN excels in precise localization, yielding higher accuracy for structured fields such as gstin and store_name (88.27% mAP50), but at slower inference speed with a model size of 315 MB; YOLOv11n yields superior recall and real-time performance, with a compact model size of 5.2 MB (92.17% mAP50), which sometimes gives less precise bounding boxes. In particular, WBF fuses the predictions of both models by clustering overlapping detections based on the IoU threshold, computing the weighted average of the bounding box coordinates and confidence scores, while applying class-specific fusion weights optimized through analysis of the validation set.

The architecture of this ensemble is designed to work on a four-stage pipeline: (1) independent-where both models process the input image in parallel and provide individual sets of bounding box prediction with class labels and confidence scores; (2) coordinate normalization-where raw pixel space boxes are converted to the [0,1] range by design for model-agnostic processing; (3) weighted box fusion-configurable IoU threshold defaults to 0.55, and for skip-box threshold, 0.0001 WEIGHTS default distributions per class favor YOLOv11n for product_table at 58% and total_amount at 70%, while Faster R-CNN has more weigh for gstin at 55% and store_name at 52%; and (4) confidence filtering-based on class thresholds to remove low-confidence detections and reduce false positives.

Implementation follows the ensemble-boxes library in Python with integrated PyTorch and Ultralytics YOLO frameworks; inference is managed through wrapper classes-FasterRCNNWrapper and YOLOWrapper-that handle model loading, prediction formatting, and coordinate transformations. Performance on the 60-image test set was significantly improved: at IoU 0.5, the ensemble reached an F1 score of 90.80% compared to 89.23% and 79.17% baselines given by YOLO and Faster R-CNN, respectively, with the best recall of 93.15% and precision of 88.63%. Per-class analysis showed that fusion effectively reduced false positives when detecting mobile_no and improved boundary precision for product_table of variable size. The final ensemble model requires both base model weights, 320 MB in total and provides a well-balanced trade-off between accuracy and flexibility for deployment in my document processing pipeline.

IV.	PIPELINE ARCHITECTURE 
The architecture is designed as a modular, 6-step, high-performance pipeline orchestrated by a central API service. At the high level, the architecture starts with the user who interacts with the system via a POST request to one of the FastAPI endpoints, which gets documented and exposed via Swagger UI. This request contains the medical bill image and triggers the 6-step processing workflow.
One of the main constituents of the architecture is the Eager Model Loading mechanism. At the time the FastAPI application fires up, it pre-loads into memory all five core models: YOLOv11n, Faster R-CNN, PaddleOCR, Table-Transformer, and MedGemma. This is to ensure that all computational resources are ready to go for inference and will not introduce any cold-start latency for the first user request or subsequent ones. The FastAPI service interfaces with the Neon PostgreSQL database for job record creation and updates, the persistence of final structured data, and state management for the processes.

A.	Stage 1: Document Upload and Initialization
The pipeline begins when a medical bill document is uploaded through the POST /api/medical-bills/process endpoint. The system accepts three image formats: JPEG, PNG, and PDF, with automatic format detection and validation. Upon receiving the upload, the system performs initial quality checks including file size constraints (maximum 10 MB), format verification through magic number inspection, and basic image integrity validation. A unique MedicalBill record is instantiated in the PostgreSQL database with an auto-incremented primary key identifier, storing metadata including original filename, file path, file size in bytes, and upload timestamp with microsecond precision. The document status is set to UPLOADED, establishing the foundation for end-to-end tracking through the pipeline. The uploaded file is temporarily stored in the system's temporary directory using Python's tempfile module with a randomly generated filename to prevent conflicts. A ProcessingJob record is concurrently created with job_type='full_pipeline' and status='PROCESSING', enabling granular monitoring of the processing workflow. This initialization stage also implements comprehensive error handling with try-catch blocks to capture upload failures, database connection errors, or file system issues, automatically setting the document status to FAILED and recording detailed error messages for debugging. The average execution time for this stage is approximately 0.2-0.5 seconds, representing less than 3% of the total processing time.

B.	Stage 2: Ensemble-Based Region Detection
Following successful initialization, the document proceeds to the region detection stage, which employs the WBF ensemble model combining YOLOv11n and Faster R-CNN. The system first loads the uploaded image using OpenCV's cv2.imread() function, converting it to a NumPy array of shape (H, W, 3) in BGR color space. A new ProcessingJob record with job_type='ensemble_detection' is created to track this stage independently. Both detection models process the image in parallel on CUDA-enabled GPU: YOLOv11n performs inference at 640×640 resolution using its anchor-free detection head with distribution focal loss, while Faster R-CNN processes the image through its ResNet-50 FPN backbone with region proposal network and ROI head. Each model returns a set of predictions formatted as tuples of (bbox, class_label, confidence_score), where bbox coordinates are in [x1, y1, x2, y2] format representing top-left and bottom-right corners. The WBF algorithm then normalizes these coordinates to the [0,1] range, clusters overlapping detections using the configured IoU threshold of 0.55, and computes weighted averages of both bounding box coordinates and confidence scores based on the class-specific fusion weights. The ensemble typically detects 7-9 regions per document, corresponding to the eight defined classes: date_of_receipt, gstin, invoice_no, mobile_no, product_table, store_address, store_name, and total_amount. Detected regions may appear multiple times when both models identify the same field with high confidence, which WBF resolves through its weighted averaging mechanism. The final ensemble predictions are filtered using class-specific confidence thresholds (default 0.5) to eliminate low-confidence false positives. Detection metadata including ensemble_model_used='Faster-RCNN + YOLO11n WBF' and the average detection_confidence across all regions are persisted to the database. This stage completes in approximately 2-3 seconds on a CUDA-enabled RTX 3050 GPU, accounting for roughly 15% of total processing time. The ProcessingJob status is updated to 'COMPLETED' upon successful detection, or 'FAILED' if the ensemble returns fewer than 3 detected regions, indicating a potentially invalid or severely degraded input image.

C.	Stage 3: Dual-Engine OCR Extraction
The third stage performs text extraction from each detected region using a sophisticated dual-engine OCR strategy. A ProcessingJob with job_type='ocr_extraction' is initialized to track this stage. For each bounding box returned by the ensemble model, the system extracts the corresponding region from the full image using NumPy array slicing with a 5-pixel padding buffer: region = image[y1-5:y2+5, x1-5:x2+5]. This padding helps capture text that may extend slightly beyond the detected boundaries. The extracted region image, typically ranging from 100×50 to 800×200 pixels depending on the field type, is first processed by PaddleOCR, which serves as the primary OCR engine. PaddleOCR executes a three-stage pipeline: (1) text detection using the DB (Differentiable Binarization) algorithm to locate text regions within the cropped area, (2) text recognition through a CRNN (Convolutional Recurrent Neural Network) architecture that converts detected text regions to character sequences, and (3) confidence scoring at the character and line level. The PaddleOCR results are returned as a list of tuples: [(bbox_coords, (text, confidence))], where bbox_coords represent the text location within the region, text is the extracted string, and confidence is a float between 0 and 1. When PaddleOCR confidence falls below the threshold of 0.6, indicating potential recognition errors on degraded, skewed, or low-contrast regions, the system automatically invokes Tesseract OCR as a fallback mechanism. Tesseract, configured with the English language model and page segmentation mode 6 (uniform block of text), processes the same region and returns its own text and confidence score. The system then compares confidence scores from both engines and selects the result with higher confidence, implementing a confidence-based arbitration strategy. One critical exception exists: the product_table region is not processed through OCR at this stage. Instead, its bounding box coordinates are stored as JSON-serialized string in the format {"x1": float, "y1": float, "x2": float, "y2": float} in the product_table_bbox field, with a reason flag indicating "Table extraction handled by post-processing." This design decision defers complex table parsing to the specialized Stage 5 pipeline. The OCR results for all other seven fields are stored in structured format, with each field assigned both a raw text value (e.g., gstin_raw) and confidence score (e.g., gstin_confidence). The system records metadata including ocr_engine_used, which may contain values like 'paddleocr', 'tesseract', or 'paddleocr_with_tesseract_fallback' depending on the engines actually utilized. This dual-engine approach achieves an overall character recognition accuracy of 95.3% on printed text and 89.7% on slightly degraded documents, substantially outperforming single-engine implementations. The OCR stage typically completes in 1-2 seconds per region, with the entire stage finishing in approximately 8-14 seconds for a typical 7-region document, representing the largest time investment at roughly 50-60% of total processing time.

D.	Stage 4: Raw Data Persistence
Upon completion of OCR extraction, the pipeline enters the database persistence stage, where raw OCR outputs are committed to PostgreSQL with status='OCR_COMPLETE'. This intermediate storage serves three critical functions: (1) enabling fault-tolerance by allowing processing resumption from this checkpoint if subsequent stages fail, (2) providing complete traceability for debugging and model refinement by preserving the original OCR text before any corrections, and (3) preventing data loss from validation failures in post-processing. A key architectural decision in this stage addresses database schema constraints: fields with strict length limitations—specifically gstin (varchar 15) and mobile_no (varchar 15)—store the raw OCR text exclusively in separate _raw columns (gstin_raw, mobile_no_raw), while their primary columns (gstin, mobile_no) are initially set to NULL. This prevents StringDataRightTruncation errors that occur when OCR misreads non-GSTIN text (e.g., "Name: DUMMY PATIENT" with 17 characters) as a GSTIN field, a common issue in variable-layout documents. The _raw fields have no length constraints, allowing storage of any OCR output up to 100 characters (after truncation for safety), preserving the complete OCR result for analysis. For all other fields—date_of_receipt_raw, invoice_no_raw, total_amount_raw, store_name_raw, store_address_raw—the system stores both the raw OCR text and initial values that will later be validated and corrected in Stage 5. Each field triplet consists of: (1) the primary field storing the final validated value, (2) the _raw field preserving original OCR output, (3) the _confidence field recording OCR confidence scores, and (4) the _bbox field storing the spatial coordinates as JSON. The full_ocr_text field concatenates all extracted text for reference and full-text search capabilities. Additionally, ensemble detection metadata including detection_confidence (average across all regions) and ensemble_model_used are persisted at this stage. The database commit operation uses SQLModel's session.add() followed by session.commit(), with comprehensive error handling that catches connection timeouts, constraint violations, and transaction conflicts, automatically triggering session.rollback() and retry logic on transient failures. For long-running database commits that exceed 30 seconds due to network latency on the serverless Neon PostgreSQL, the system implements exponential backoff retry with a maximum of 3 attempts. This persistence stage typically completes in 0.5-1.0 seconds, consuming approximately 5% of total processing time, and establishes a stable checkpoint from which the pipeline can safely proceed to validation and correction.

E.	Stage 5: Post-Processing, Validation, and LLM Correction
The fifth stage represents the most sophisticated component of the pipeline, implementing a hybrid validation and correction workflow that combines rule-based validators with LLM-powered semantic corrections. A ProcessingJob with job_type='post_processing' is created to track this stage. The post-processing pipeline orchestrates field-specific correction through eight specialized FieldCorrector classes, each implementing a three-step correction protocol: rule-based validation, conditional LLM correction, and re-validation with confidence scoring.

The rule-based validation layer applies domain-specific constraints through five validator classes: (1) GST_Validator enforces the 15-character alphanumeric GSTIN format (pattern: ^\d{2}[A-Z]{5}\d{4}[A-Z]{1}[1-9A-Z]{1}Z[0-9A-Z]{1}$) with state code verification (01-37), returning is_valid=True only when all format constraints are satisfied; (2) MobileNumberValidator extracts 10-digit sequences starting with digits 6-9 from potentially concatenated text (e.g., "9099823566,90998235" → "9099823566"), handling common OCR artifacts like spaces and punctuation; (3) DateValidator recognizes multiple input formats (DD-MM-YYYY, DD/MM/YYYY, DD.MM.YYYY, YYYY-MM-DD) and normalizes all to the ISO 8601 format YYYY-MM-DD for database storage, parsing the string to a Python date object; (4) AmountValidator removes currency symbols (₹, $, Rs, INR) while critically preserving decimal points, correcting a common error where regex patterns like [₹$Rs\.INR\s] inadvertently remove decimal separators, causing amounts like 328.58 to become 32858; (5) InvoiceNumberValidator performs basic alphanumeric validation with length constraints (5-50 characters). Each validator returns a structured dictionary containing is_valid (boolean), corrected_value (cleaned/formatted text or None), confidence (float 0-1 based on validation success), errors (list of error messages), and needs_llm_correction (boolean flag for low-confidence cases).

When validation confidence falls below thresholds (typically <0.5) or fails entirely, the pipeline invokes Google's MedGemma 4B IT, a 4-bit quantized multimodal large language model fine-tuned for medical domain understanding. The LLM integration utilizes the Hugging Face Transformers library with BitsAndBytesConfig for 4-bit NF4 quantization and double quantization, reducing model memory footprint from ~16GB to ~2GB on GPU. MedGemma is loaded once at application startup and cached for all subsequent requests, eliminating repeated model loading overhead. The LLM corrector implements field-specific correction methods: (1) correct_gstin() submits poorly formatted GSTIN candidates with prompts requesting format standardization; (2) correct_mobile_number() handles digit extraction from noisy text; (3) correct_store_name() applies sophisticated formatting rules—expanding abbreviations (PVT → Pvt. Ltd., INT → International), applying Title Case, and correcting spelling errors—through a carefully engineered prompt: "Fix spelling, add spacing between words, complete abbreviations (PVT → Pvt. Ltd.), use Title Case (capitalize first letter of each word). Input: {raw_text}. Return ONLY the corrected name, no explanations"; (4) correct_store_address() improves readability by inserting commas after street names and areas, fixing spacing issues, and completing common abbreviations (FLCOR → Floor, OPP → Opposite) via prompt: "Add commas after street/area, fix spacing, correct abbreviations (FLCOR → Floor, OPP → Opposite). Input: {raw_text}. Return ONLY the formatted address." The LLM generates responses using the apply_chat_template() method with system and user messages, constrained to a maximum of 50 tokens for field corrections to ensure concise outputs. All LLM operations execute within torch.inference_mode() context for computational efficiency, with response parsing that extracts only the corrected text while discarding any explanatory preambles or formatting artifacts.

Corrected values undergo re-validation using the same rule-based validators to ensure LLM outputs meet format requirements. The system assigns correction_method tags: 'validator' for values passing initial validation, 'llm' for LLM-corrected values that pass re-validation, and 'validator_failed' for values that fail even after LLM attempts. Final confidence scores combine OCR confidence, validation confidence, and correction method through multiplicative weighting (e.g., LLM corrections receive a 0.9 multiplier as a slight penalty). Critically, when validation fails after all correction attempts, the system stores None in the primary field rather than invalid data, maintaining database integrity by preventing constraint violations. The _update_field() method implements this logic: if corrected_value is None, it skips the database update entirely, printing "⚠ Skipping update for {field_name} (value is None)" to preserve any existing valid data from previous processing attempts.

The post-processing pipeline also handles the specialized case of product_table extraction. When a product_table bbox exists and is not marked as skipped, the system invokes the intelligent LLM-based table parser. Unlike traditional table extraction approaches that rely on grid line detection through OpenCV's Hough transforms or table structure models that require visible cell boundaries, the proposed method leverages spatial-semantic understanding through MedGemma to handle borderless tables ubiquitous in real-world medical bills. The table extraction workflow proceeds as follows: (1) the table region image is extracted using the stored bbox coordinates; (2) PaddleOCR processes the entire table region, returning text blocks with their spatial coordinates as tuples of [(bbox, (text, confidence))]; (3) text blocks are grouped into rows using y-coordinate proximity clustering with a threshold of 15 pixels—blocks with center y-coordinates within this threshold are considered part of the same row; (4) within each row, text blocks are sorted by x-coordinate to establish left-to-right reading order; (5) the structured representation (rows with ordered text) is formatted as a plain-text table and submitted to MedGemma with a carefully engineered 500-token prompt specifying the extraction task. The prompt explicitly instructs: "You are analyzing a medical bill product table. Extract each product as a structured JSON object. Identify which columns contain: product name, quantity, pack/pack size, MRP/price, expiry date, and total amount. Return ONLY a valid JSON array of product objects in this exact format: [{'product': 'full product name', 'quantity': '...', 'pack': '...', 'mrp': '...', 'expiry': '...', 'total_amount': '...'}]. CRITICAL Rules: (1) Product name: Use FULL text from Product Description column, NOT company code; (2) total_amount: MUST be LAST/RIGHTMOST numeric value in row (final payable amount), NOT Net Rate or MRP; (3) Skip header rows completely; (4) Return ONLY valid JSON array, no explanations." This prompt engineering addresses two critical failure modes observed during development: (1) extracting company codes like "SYST" instead of full product names like "AF 400 TAB" and (2) confusing intermediate amounts (Net Rate, MRP) with the final line total. MedGemma's multimodal capabilities enable it to understand spatial relationships without explicit column headers, effectively mapping text to semantic categories (product name, quantity, etc.) based on position, length, and context. The LLM response undergoes robust JSON parsing with three fallback strategies: (1) direct JSON decoding using json.loads(), (2) regex-based array extraction with pattern \[\s*\{.*?\}\s*\], and (3) individual object extraction with pattern \{[^{}]*\}, handling variations in LLM output formatting including markdown code blocks, explanatory text, or malformed JSON. Successfully parsed product objects are instantiated as ProductItem records with foreign key medical_bill_id, storing six fields (product, quantity, pack, mrp, expiry, total_amount) along with row_index for ordering. The system applies _parse_amount() to convert string amounts like "1251.0" or "₹328.58" to float values through regex [^\d.]+ for currency symbol removal. All product items are bulk-inserted via session.add() in a single transaction, with session.commit() persisting them atomically. The table extraction subprocess achieves 90.1% accuracy on product name extraction and 92.3% on amount extraction across varying table layouts, substantially outperforming traditional grid-based methods (67.4% accuracy) on borderless tables.

The entire post-processing stage, including field validation, LLM corrections, and table extraction, typically completes in 3-5 seconds for a document with 7 fields and 4 product rows, accounting for approximately 25% of total processing time. The system tracks detailed metrics including fields_corrected (count of successful validations), llm_corrections (count of LLM invocations), and product_items_extracted (count of parsed table rows), all persisted as metadata for performance analysis. Database commits implement retry logic with exponential backoff to handle transient connection failures on the serverless PostgreSQL instance, with session.rollback() triggered on any exception to maintain transaction consistency.

F.	Stage 6: Finalization and Status Update
The sixth and final stage performs status finalization, comprehensive error aggregation, and response generation. Upon successful completion of all previous stages, the document status is atomically updated from 'OCR_COMPLETE' to 'COMPLETED' through a database transaction, with processing_timestamp set to the current UTC time. The system constructs a detailed JSON response containing: (1) the complete MedicalBill object with all extracted and validated fields, (2) a detection_summary dictionary with ensemble predictions for each of the eight classes including bbox coordinates, class labels, and confidence scores, (3) an ocr_results dictionary mapping field names to their raw OCR text, engine used, and confidence scores, (4) a post_processing dictionary containing fields_corrected, llm_corrections, and product_items_extracted counts, and (5) a product_items array with all extracted table rows, each formatted as {product, quantity, pack, mrp, expiry, total_amount}. The response excludes raw binary data and internal IDs, applying Pydantic serialization with exclude={'medical_bill': {'processing_jobs'}} to prevent circular references. If any stage encounters an exception, the pipeline implements graceful degradation: (1) the ProcessingJob for the failed stage is marked as status='FAILED' with detailed error_message capturing the exception traceback; (2) the MedicalBill status is set to 'FAILED' with validation_notes containing a summary of the failure point; (3) session.rollback() is invoked to prevent partial data commits; (4) an error response is returned with HTTP status code 500 and a JSON body containing {"detail": "Processing failed: {error_message}"} to inform the client of the specific failure. The system logs comprehensive error traces using Python's traceback module, enabling post-mortem debugging and model refinement. This finalization stage completes in approximately 0.3-0.5 seconds, representing less than 3% of total processing time.

The complete six-stage pipeline achieves end-to-end processing times of 15-20 seconds per document on an NVIDIA RTX 3050 GPU (6GB VRAM) with 16GB system RAM, with stage-wise time distribution: Stage 1 (2-3%), Stage 2 (15-18%), Stage 3 (50-60%), Stage 4 (3-5%), Stage 5 (20-25%), and Stage 6 (2-3%). The modular architecture with independent ProcessingJob tracking for each stage enables fine-grained performance monitoring, bottleneck identification, and selective retry on stage failures, establishing a production-ready foundation for high-throughput medical document processing.
